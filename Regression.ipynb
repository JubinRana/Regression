{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1/ What is SLR.\n",
        "\n",
        "SLR stands for Simple Linear Regression.\n",
        "\n",
        "Simple Linear Regression is a statistical method that allows us to study and summarize the relationship between two continuous (quantitative) variables:\n",
        "\n",
        "Independent variable (X): This is the predictor variable.\n",
        "Dependent variable (Y): This is the response or outcome variable, and we want to predict it based on the independent variable."
      ],
      "metadata": {
        "id": "x8fGIOTsh-cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2/ what are key assumptions of SLR.\n",
        "-\n",
        "Linearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear. This means that the change in Y is proportional to the change in X.\n",
        "\n",
        "Independence: The observations are independent of each other. This means that the value of one observation does not affect the value of another observation.\n",
        "\n",
        "Homoscedasticity: The variance of the errors (residuals) is constant across all levels of the independent variable. This means that the spread of the data points around the regression line is roughly the same for all values of X.\n",
        "\n",
        "Normality: The errors (residuals) are normally distributed. This means that the distribution of the errors follows a bell-shaped curve.\n",
        "\n"
      ],
      "metadata": {
        "id": "UTXae0W321S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/ what does---------y=mx+c.\n",
        "- In the equation of a straight line, y = mx + c, the coefficient 'm' represents the slope or gradient of the line.\n",
        "\n",
        "Slope/Gradient:\n",
        "\n",
        "The slope indicates the steepness and direction of the line. It represents the change in the dependent variable (y) for a unit change in the independent variable (x)."
      ],
      "metadata": {
        "id": "_QnKkbY222Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/ What does the intercept c in eq. y=mx+c.\n",
        "\n",
        "- In the equation of a straight line, y = mx + c, the intercept 'c' represents the y-intercept of the line.\n",
        "\n",
        "Y-intercept:\n",
        "\n",
        "The y-intercept is the point where the line crosses the y-axis. It is the value of y when x is equal to 0."
      ],
      "metadata": {
        "id": "ea6M6-Wn22qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5/ How do we calculate slope m in SLR.\n",
        "\n",
        "- The slope 'm' in SLR, also known as the regression coefficient, represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
        "\n",
        "Here's how to calculate the slope 'm':\n",
        "\n",
        "Formula:\n",
        "\n",
        "m = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]"
      ],
      "metadata": {
        "id": "yEnl4M1l23Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6/ What is the purpose of least square method in SLR.\n",
        "\n",
        "- The least squares method is used to determine the equation of this best-fitting line.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "The main purpose of the least squares method is to minimize the sum of the squared differences (errors or residuals) between the observed values of Y and the values predicted by the regression line."
      ],
      "metadata": {
        "id": "jOOh7sw224A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7/ How is coeff. of determination interpreted in SLR.\n",
        "- Interpretation:\n",
        "\n",
        "R² values range from 0 to 1, and they are interpreted as follows:\n",
        "\n",
        "R² = 0: This indicates that the model explains none of the variability of the response data around its mean. The independent variable (X) does not help in predicting the dependent variable (Y).\n",
        "\n",
        "R² = 1: This indicates that the model explains all the variability of the response data around its mean. The independent variable (X) perfectly predicts the dependent variable (Y).\n",
        "\n",
        "0 < R² < 1: This indicates the proportion of the variance in the dependent variable that is predictable from the independent variable. For example, an R² of 0.75 means that 75% of the variance in Y can be explained by X."
      ],
      "metadata": {
        "id": "Lj1cfnqO24Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8/ What is MLR.\n",
        "- MLR stands for Multiple Linear Regression.\n",
        "\n",
        "Multiple Linear Regression is a statistical technique that uses two or more independent variables to predict the outcome of a single dependent variable. It's an extension of Simple Linear Regression (SLR), which uses only one independent variable."
      ],
      "metadata": {
        "id": "6xbxJIIu243k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9/ What is main difference between SLR and MLR.\n",
        "\n",
        "- The primary difference lies in the number of independent variables used to predict the dependent variable:\n",
        "\n",
        "SLR: Uses only one independent variable to predict the dependent variable.\n",
        "\n",
        "MLR: Uses two or more independent variables to predict the dependent variable."
      ],
      "metadata": {
        "id": "__5mtUOU25cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10/ What are the key assumptions of MLR.\n",
        "- Linearity: There should be a linear relationship between the dependent variable and each independent variable. This means that the change in the dependent variable is proportional to the change in the independent variable.\n",
        "\n",
        "Independence: The observations should be independent of each other. This means that the value of one observation does not affect the value of another observation. This is often violated in time series data, where observations are dependent on previous observations.\n",
        "\n",
        "Homoscedasticity: The variance of the errors (residuals) should be constant across all levels of the independent variables. This means that the spread of the data points around the regression line is roughly the same for all values of the independent variables."
      ],
      "metadata": {
        "id": "AKV6qc_226Mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11/ What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "- Heteroscedasticity refers to the situation where the variance of the errors (residuals) in a regression model is not constant across all levels of the independent variables. In simpler terms, the spread or variability of the data points around the regression line is not consistent.\n",
        "\n",
        "Effects on MLR: Heteroscedasticity can lead to inefficient and biased estimates of the regression coefficients, inflated standard errors, and unreliable hypothesis tests. This means that the model's predictions and inferences may not be accurate."
      ],
      "metadata": {
        "id": "kWOauXf2-KKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12/ How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "- Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other. This can make it difficult to determine the individual effects of each independent variable on the dependent variable.\n",
        "Improvements:\n",
        "Remove one or more of the highly correlated variables: If two variables are very similar, consider removing one from the model.\n",
        "Combine highly correlated variables: Create a composite variable by combining the information from the correlated variables.\n",
        "Use regularization techniques: Techniques like Ridge Regression or Lasso Regression can help to reduce the impact of multicollinearity by shrinking the coefficients towards zero."
      ],
      "metadata": {
        "id": "Ixao1frF-KnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13/ What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- One-hot encoding: Create dummy variables for each category of the categorical variable. This is the most common approach.\n",
        "Label encoding: Assign a numerical value to each category. This can be useful for ordinal categorical variables where there is a natural order to the categories.\n",
        "Target encoding: Replace each category with the mean of the dependent variable for that category. This can be useful for improving predictive performance but should be used with caution to avoid overfitting."
      ],
      "metadata": {
        "id": "BEzvcWC8-LFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14/ What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "- Interaction terms are used to model the combined effect of two or more independent variables on the dependent variable. They capture situations where the effect of one independent variable on the dependent variable depends on the value of another independent variable.\n",
        "Role: Interaction terms allow for more complex and nuanced relationships between variables to be modeled, improving the accuracy and interpretability of the model."
      ],
      "metadata": {
        "id": "ATLtgIsM-LlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15/ How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "- SLR: The intercept represents the predicted value of the dependent variable when the independent variable is 0.\n",
        "MLR: The intercept represents the predicted value of the dependent variable when all independent variables are 0. This interpretation may not always be meaningful in the context of the data, especially if the independent variables cannot realistically take on the value of 0."
      ],
      "metadata": {
        "id": "rtTlBBPm-MDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16/ What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "- Significance: The slope represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant. It indicates the direction and strength of the relationship between the variables.\n",
        "Effect on predictions: The slope is used to calculate the predicted values of the dependent variable based on the values of the independent variables. A larger slope indicates a greater change in the dependent variable for a given change in the independent variable."
      ],
      "metadata": {
        "id": "TKxKCeDj-Mij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17/ How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "- Context: The intercept provides a baseline value for the dependent variable when all independent variables are 0. It helps to understand the starting point of the relationship between the variables."
      ],
      "metadata": {
        "id": "r-OlRi1u_NKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "18/ What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "- Limitations:\n",
        "R² does not indicate whether the model is a good fit for the data in terms of assumptions like linearity, homoscedasticity, or normality.\n",
        "\n",
        "R² can be artificially inflated by adding more independent variables to the model, even if they are not truly relevant. Adjusted R² is a modified version that accounts for the number of predictors in the model.\n",
        "\n",
        "R² should be interpreted in the context of the specific data and research question."
      ],
      "metadata": {
        "id": "cZNtreTV_Ns3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19/ How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "- Interpretation: A large standard error for a regression coefficient indicates that there is a lot of uncertainty about the true value of the coefficient. This could be due to a small sample size, high variability in the data, or multicollinearity. It suggests that the coefficient may not be statistically significant and that the model's predictions may not be very precise."
      ],
      "metadata": {
        "id": "Q1pdGJnn_ONL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20/ How can heteroscedasticity be identified in residual plots, and why is it important to address?\n",
        "- Identification: In residual plots, heteroscedasticity is often indicated by a cone or fan-shaped pattern, where the spread of the residuals increases or decreases as the fitted values increase.\n",
        "Importance: Heteroscedasticity can lead to inefficient and biased estimates of the regression coefficients, making the model's predictions and inferences unreliable. Addressing heteroscedasticity can improve the accuracy and validity of the model."
      ],
      "metadata": {
        "id": "eSLhCN5o_OqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21/ What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "- High R² but Low Adjusted R²: This typically indicates that the model has too many irrelevant or unnecessary predictors. R² tends to increase as you add more predictors, even if they don't significantly improve the model's explanatory power. Adjusted R², on the other hand, penalizes the addition of irrelevant predictors.\n",
        "Interpretation: A high R² with a low adjusted R² suggests that the model may be overfitting the data – it's capturing noise rather than true relationships. This can lead to poor generalization to new data."
      ],
      "metadata": {
        "id": "t7Z2EmjlDU4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22/ Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "- Scaling, such as standardization or normalization, brings variables to a similar scale or range.\n",
        "Importance:\n",
        "Prevents Dominance: In MLR, variables with larger scales can disproportionately influence the model. Scaling ensures that all variables have a similar impact.\n",
        "\n",
        "Improves Algorithm Performance: Many optimization algorithms used in regression (e.g., gradient descent) converge faster and more reliably when variables are scaled.\n"
      ],
      "metadata": {
        "id": "rTbcm1vwDVVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23/ What is polynomial regression?\n",
        "\n",
        "- Polynomial Regression: It's a form of regression analysis where the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth-degree polynomial. It extends linear regression by allowing for curved relationships."
      ],
      "metadata": {
        "id": "tmN6q4rwDV9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24/ How does polynomial regression differ from linear regression?\n",
        "\n",
        "- Linear Regression: Assumes a linear relationship between variables (a straight line).\n",
        "Polynomial Regression: Models a non-linear relationship using polynomial terms (curves). It introduces higher-order powers of the independent variable (x², x³, etc.) into the regression equation."
      ],
      "metadata": {
        "id": "DEYWMR5JDWe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25/ When is polynomial regression used?\n",
        "\n",
        "- Non-linear Relationships: When data shows a curved or non-linear pattern, polynomial regression is more appropriate than linear regression.\n",
        "Capturing Complexity: It can capture more complex relationships between variables, providing a better fit to the data."
      ],
      "metadata": {
        "id": "IfEdDFeXDW_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26/ What is the general equation for polynomial regression?\n",
        "- y = b0 + b1x + b2x² + b3x³ + ... + bnxⁿ + ε\n",
        "\n",
        "where:\n",
        "\n",
        "y: dependent variable\n",
        "x: independent variable\n",
        "b0, b1, b2, ..., bn: regression coefficients\n",
        "n: degree of the polynomial\n",
        "ε: error term"
      ],
      "metadata": {
        "id": "aA4wUPxHDXfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 27/ Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "- Yes: Polynomial regression can be extended to include multiple independent variables, similar to multiple linear regression. Interaction terms and higher-order terms of multiple variables can be included."
      ],
      "metadata": {
        "id": "w9g0nCKGDYgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28/ What are the limitations of polynomial regression?\n",
        "\n",
        "- Overfitting: Higher-degree polynomials can overfit the data, capturing noise rather than true patterns.\n",
        "Extrapolation: Polynomial models can be unreliable for predicting values outside the range of the observed data.\n",
        "Interpretability: Higher-degree polynomials can be difficult to interpret, especially with multiple variables."
      ],
      "metadata": {
        "id": "r1i4fOZ7DZbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 29/ What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "- Cross-validation: Divide the data into training and validation sets. Train the model with different polynomial degrees on the training set and evaluate performance on the validation set.\n",
        "Information Criteria: Use metrics like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to compare models with different degrees. Lower values indicate better model fit.\n",
        "Adjusted R²: Look for the degree that maximizes adjusted R² while avoiding overfitting."
      ],
      "metadata": {
        "id": "0pOIzjzQDZ8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30/ Why is visualization important in polynomial regression?\n",
        "\n",
        "- Understanding Relationships: Visualization helps to see the relationship between the variables and assess whether a polynomial model is appropriate.\n",
        "Detecting Overfitting: Visualizing the fitted curve against the data can reveal overfitting if the curve is overly complex and tries to pass through every data point.\n",
        "Choosing the Degree: Visualization can guide the selection of the polynomial degree by showing how well different degrees fit the data."
      ],
      "metadata": {
        "id": "u4F6AgMGDalU"
      }
    }
  ]
}